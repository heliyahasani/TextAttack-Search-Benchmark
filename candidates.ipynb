{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synsets found for complain.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "\n",
    "# Ensure all necessary NLTK data is downloaded\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "word_dict ={}\n",
    "# Function to map NLTK's POS tags to WordNet's POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN  # Default to noun if not found\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Clean and normalize a word by removing non-alphanumeric characters\n",
    "def clean_word(word):\n",
    "    return ''.join(char for char in word if char.isalnum())\n",
    "\n",
    "# Process the sentence: clean it, remove stopwords, lemmatize, and find synonyms\n",
    "sentence = \"Such an amazing movie cannot complain.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize, clean, and remove stopwords\n",
    "words = [clean_word(word) for word in word_tokenize(sentence.lower()) if word.lower() not in stop_words and word.isalnum()]\n",
    "\n",
    "# POS tagging\n",
    "tagged_words = pos_tag(words)\n",
    "\n",
    "# Lemmatize words based on their POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
    "\n",
    "# Function to find synonyms and calculate similarity\n",
    "def find_synonyms_and_similarity(word, pos):\n",
    "    base_synsets = wn.synsets(word, pos=pos)\n",
    "    if not base_synsets:\n",
    "        print(f\"No synsets found for {word}.\")\n",
    "        return\n",
    "    \n",
    "    base_synset = base_synsets[0]  # Use the first synset as the base for comparison\n",
    "    synonyms = set()\n",
    "    similarities = {}\n",
    "\n",
    "    for synset in wn.synsets(word, pos=pos):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonym = lemma.name()\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "                synonym_synsets = wn.synsets(synonym, pos=pos)\n",
    "                if synonym_synsets:\n",
    "                    similarity = base_synset.wup_similarity(synonym_synsets[0])\n",
    "                    if similarity is not None:\n",
    "                        similarities[synonym] = similarity\n",
    "    \n",
    "    for synonym, similarity in similarities.items():\n",
    "        if word_dict.get(word) is None:\n",
    "            word_dict[word] = [] \n",
    "        word_dict[word].append([synonym,similarity])\n",
    "# Print synonyms and similarity scores for each lemmatized word\n",
    "for word, tag in tagged_words:\n",
    "    wordnet_pos = get_wordnet_pos(tag)\n",
    "    find_synonyms_and_similarity(word, wordnet_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_with_similar_word(sentence, synonym_dict):\n",
    "    \"\"\"Replace a random word in the sentence with its most similar synonym based on similarity scores.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence where a word will be replaced.\n",
    "        synonym_dict (dict): A dictionary where keys are words from the sentence and \n",
    "                             values are lists of tuples (synonym, similarity_score).\n",
    "\n",
    "    Returns:\n",
    "        str: The updated sentence with a word replaced by its most similar synonym.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If synonym_dict is empty or does not contain synonyms for any word in the sentence.\n",
    "    \"\"\"\n",
    "    if not synonym_dict:\n",
    "        raise ValueError(\"The synonym dictionary is empty, no synonyms to choose from.\")\n",
    "\n",
    "    # Split the sentence into words\n",
    "    sentence_words = sentence.split()\n",
    "\n",
    "    # Choose a random word from the sentence that has a synonym\n",
    "    words_with_synonyms = [word for word in sentence_words if word in synonym_dict]\n",
    "    if not words_with_synonyms:\n",
    "        raise ValueError(\"No words in the sentence have synonyms in the dictionary.\")\n",
    "\n",
    "    word_to_replace = random.choice(words_with_synonyms)\n",
    "\n",
    "    # Choose the synonym with the highest similarity score\n",
    "    synonyms_with_scores = synonym_dict[word_to_replace]\n",
    "    replacement_word = max(synonyms_with_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Replace the word in the sentence\n",
    "    updated_sentence = ' '.join([\n",
    "        replacement_word if word == word_to_replace else word\n",
    "        for word in sentence_words\n",
    "    ])\n",
    "\n",
    "    return updated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    \"\"\"Return a cleaned version of the word.\"\"\"\n",
    "    return ''.join(char for char in word if char.isalnum()).lower()\n",
    "\n",
    "def replace_specific_word(sentence, word_to_replace, replacement_word):\n",
    "    \"\"\"Replace a specific word in the sentence with the provided replacement word.\"\"\"\n",
    "    words = sentence.split()\n",
    "    replaced_sentence = ' '.join([replacement_word if clean_word(word) == clean_word(word_to_replace) else word for word in words])\n",
    "    return replaced_sentence\n",
    "\n",
    "def focused_tabu_search(sentence, word_dict, target_word):\n",
    "    \"\"\"Perform a focused tabu search on a single word's synonyms.\"\"\"\n",
    "    cleaned_word = clean_word(target_word)\n",
    "    \n",
    "    if cleaned_word not in word_dict:\n",
    "        print(f\"No synonyms found for {target_word}.\")\n",
    "        return sentence\n",
    "    \n",
    "    synonyms = [syn for syn in word_dict[cleaned_word] if syn[1] < 1.0]\n",
    "    synonyms = sorted(synonyms, key=lambda x: x[1], reverse=True)  # Sort synonyms by similarity\n",
    "    \n",
    "    for synonym, _ in synonyms:\n",
    "        modified_sentence = replace_specific_word(sentence, target_word, synonym)\n",
    "        print(f\"Replacing '{target_word}' with '{synonym}': {modified_sentence}\")\n",
    "    \n",
    "    return modified_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target word for replacement: movie\n",
      "Replacing 'movie' with 'picture': Such an amazing picture cannot complain.\n",
      "Target word for replacement: amazing\n",
      "Replacing 'amazing' with 'get': Such an get picture cannot complain.\n",
      "No eligible target word found or all words are in the tabu list.\n",
      "Final Modified Sentence: ('Such an get picture cannot complain.', ['stick'])\n"
     ]
    }
   ],
   "source": [
    "def clean_word(word):\n",
    "    \"\"\"Clean and normalize a word.\"\"\"\n",
    "    return ''.join(char for char in word if char.isalnum()).lower()\n",
    "\n",
    "def replace_specific_word(sentence, word_to_replace, replacement_word):\n",
    "    \"\"\"Replace a specific word in the sentence with the provided replacement word.\"\"\"\n",
    "    words = sentence.split()\n",
    "    replaced_sentence = ' '.join([replacement_word if clean_word(word) == clean_word(word_to_replace) else word for word in words])\n",
    "    return replaced_sentence\n",
    "\n",
    "def focused_tabu_search(sentence, word_dict, max_tabu_size=5):\n",
    "    \"\"\"Perform a focused tabu search on synonyms of words in the sentence, with tabu list and rule.\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    tabu_list = []\n",
    "    \n",
    "    for _ in range(len(words)):  # Limit iterations to the number of words to prevent infinite loops\n",
    "        target_word = choose_random_target_word(sentence, word_dict, tabu_list)\n",
    "        if not target_word:\n",
    "            print(\"No eligible target word found or all words are in the tabu list.\")\n",
    "            break\n",
    "\n",
    "        cleaned_word = clean_word(target_word)\n",
    "        print(f\"Target word for replacement: {target_word}\")\n",
    "        \n",
    "        # Filter synonyms to exclude those in the tabu list\n",
    "        synonyms = [syn for syn in word_dict[cleaned_word] if syn[1] < 1.0 and syn[0] not in tabu_list]\n",
    "        synonyms = sorted(synonyms, key=lambda x: x[1], reverse=True)  # Sort synonyms by similarity\n",
    "        \n",
    "        if synonyms:\n",
    "            synonym, _ = synonyms[0]\n",
    "            sentence = replace_specific_word(sentence, target_word, synonym)\n",
    "            print(f\"Replacing '{target_word}' with '{synonym}': {sentence}\")\n",
    "            \n",
    "            # Update tabu list\n",
    "            tabu_list.append(cleaned_word)  # Add the replaced word to tabu list\n",
    "            tabu_list.extend([syn[0] for syn in synonyms])  # Add synonyms to tabu list\n",
    "            tabu_list = tabu_list[-1:]\n",
    "\n",
    "    return sentence,tabu_list\n",
    "\n",
    "def choose_random_target_word(sentence, word_dict, tabu_list):\n",
    "    \"\"\"Choose a random target word from the sentence that is present in the word_dict and not in the tabu list.\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    eligible_words = [word for word in words if clean_word(word) in word_dict and clean_word(word) not in tabu_list]\n",
    "    if not eligible_words:\n",
    "        return None\n",
    "    return random.choice(eligible_words)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "modified_sentence = focused_tabu_search(sentence, word_dict)\n",
    "print(\"Final Modified Sentence:\", modified_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Example training data\n",
    "X_train = ['This is a great movie', 'I hated this movie', 'This was a great experience', 'I love this book']\n",
    "y_train = ['positive', 'negative', 'positive', 'positive']\n",
    "\n",
    "# Train the classifier\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "def model_predict(sentence):\n",
    "    \"\"\"Predict the class of a given sentence.\"\"\"\n",
    "    prediction = model.predict([sentence])\n",
    "    return prediction[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_example(sentence, word_dict, model_predict, max_iterations=10):\n",
    "    original_prediction = model_predict(sentence)\n",
    "    print(f\"Original sentence: '{sentence}' predicted as {original_prediction}\")\n",
    "    \n",
    "    modified_sentence = sentence\n",
    "    tabu_list = []\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iterations:\n",
    "        # Assume focused_tabu_search is adapted to return a tuple (modified_sentence, word_replaced)\n",
    "        modified_sentence, word_replaced = focused_tabu_search(modified_sentence, word_dict, tabu_list)\n",
    "        \n",
    "        if not word_replaced:  # If no word was replaced, stop the iteration\n",
    "            print(\"No more words left to replace.\")\n",
    "            break\n",
    "        \n",
    "        new_prediction = model_predict(modified_sentence)\n",
    "        print(f\"Modified sentence: '{modified_sentence}' predicted as {new_prediction}\")\n",
    "        \n",
    "        if new_prediction != original_prediction:\n",
    "            print(\"Adversarial example found!\")\n",
    "            return modified_sentence\n",
    "        \n",
    "        iterations += 1\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: 'Such an amazing movie cannot complain.' predicted as positive\n",
      "Target word for replacement: amazing\n",
      "Replacing 'amazing' with 'get': Such an get movie cannot complain.\n",
      "Target word for replacement: movie\n",
      "Replacing 'movie' with 'picture': Such an get picture cannot complain.\n",
      "No eligible target word found or all words are in the tabu list.\n",
      "Modified sentence: 'Such an get picture cannot complain.' predicted as positive\n",
      "No eligible target word found or all words are in the tabu list.\n",
      "No more words left to replace.\n"
     ]
    }
   ],
   "source": [
    "generate_adversarial_example(sentence, word_dict, model_predict, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
