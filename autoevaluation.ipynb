{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is used to evaluate the adversarial examples generated by each attack methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 08:22:44.159546: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 08:22:44.163942: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 08:22:44.219192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import notebook as tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PYTORCH_DEVICE = 0\n",
    "TF_DEVICE = 1\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GPT2Metric` measures the percent difference is perplexities of original text $x$ and adversarial example $x_{adv}$.\n",
    "\n",
    "`USEMetric` measures the Universal Sentence Encoder similarity between $x$ and $x_{adv}$.\n",
    "\n",
    "`PercentageOfWordsChanged`: measures the percentage of words swapped in $x$ to produce $x_{adv}$. \n",
    "\n",
    "`Evaluator`: evaluator runs all three metrics for each sample and reports the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT2Metric:\n",
    "    def __init__(self):\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        self._model.to(device=f'cuda:{PYTORCH_DEVICE}')\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "        \n",
    "    def perplexity(self, text):\n",
    "        input_ids = self._tokenizer.encode(text)\n",
    "        input_ids = input_ids[: self._tokenizer.model_max_length - 2]\n",
    "        input_ids.insert(0, self._tokenizer.bos_token_id)\n",
    "        input_ids.append(self._tokenizer.eos_token_id)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        input_ids = input_ids.to(device=f'cuda:{PYTORCH_DEVICE}')\n",
    "        with torch.no_grad():\n",
    "            loss = self._model(input_ids, labels=input_ids)[0].item()\n",
    "    \n",
    "        perplexity = math.exp(loss)\n",
    "        return perplexity\n",
    "    \n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_perplexity = self.perplexity(orig_text)\n",
    "        new_perplexity = self.perplexity(new_text)\n",
    "        return (new_perplexity - orig_perplexity) / orig_perplexity\n",
    "    \n",
    "\n",
    "class USEMetric:\n",
    "    def __init__(self):\n",
    "        tfhub_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "        with tf.device(f'/device:GPU:{TF_DEVICE}'):\n",
    "            self._model = hub.load(tfhub_url)\n",
    "\n",
    "    def encode(self, orig_text, new_text):\n",
    "        with tf.device(f'/device:GPU:{TF_DEVICE}'):\n",
    "            return self._model([orig_text, new_text]).numpy()\n",
    "    \n",
    "    def get_angular_sim(self, emb1, emb2):\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=0)(emb1, emb2)\n",
    "        return 1 - (torch.acos(cos_sim) / math.pi)\n",
    "    \n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_emb, new_emb = self.encode(orig_text, new_text)\n",
    "        orig_emb = torch.tensor(orig_emb)\n",
    "        new_emb = torch.tensor(new_emb)\n",
    "        sim = self.get_angular_sim(orig_emb, new_emb).item()\n",
    "        return sim\n",
    "\n",
    "class PercentageOfWordsChanged:\n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_words = np.array(orig_text.split())\n",
    "        new_words = np.array(new_text.split())\n",
    "        words_changed = (orig_words != new_words).sum()\n",
    "        return words_changed * 100 / len(orig_words)\n",
    "    \n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.use_metric = USEMetric()\n",
    "        self.gpt2_metric = GPT2Metric()\n",
    "        self.percentageOfWordsChanged = PercentageOfWordsChanged()\n",
    "        \n",
    "    def evaluate(self, csv_file, all_successful):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df[df['result_type']==\"Successful\"]\n",
    "\n",
    "        total_sim = 0\n",
    "        total_pp_diff = 0\n",
    "        word_changed_percent = 0\n",
    "        N = 0\n",
    "        for i, row in df.iterrows():\n",
    "            original_text = row[\"original_text\"].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            if original_text not in all_successful:\n",
    "                continue\n",
    "            perturbed_text = row[\"perturbed_text\"].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            sim = self.use_metric.calc_metric(original_text, perturbed_text)\n",
    "            total_sim += sim\n",
    "            pp_diff = self.gpt2_metric.calc_metric(original_text, perturbed_text)\n",
    "            total_pp_diff += pp_diff\n",
    "            word_changed_percent += self.percentageOfWordsChanged.calc_metric(original_text, perturbed_text)\n",
    "            N += 1\n",
    "\n",
    "        return total_sim / N, total_pp_diff / N, word_changed_percent / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 08:22:46.416531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 08:22:46.421325: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [\"bert-yelp-test\", \"bert-mr-test\", \"bert-snli-test\", \"lstm-yelp-test\", \"lstm-mr-test\"]\n",
    "model_dataset_names = {\n",
    "    \"bert-mr-test\": \"BERT Movie Reviews\",\n",
    "    \"bert-yelp-test\": \"BERT Yelp Polarity\",\n",
    "    \"lstm-mr-test\": \"LSTM Movie Reviews\",\n",
    "    \"lstm-yelp-test\": \"LSTM Yelp Polarity\",\n",
    "}\n",
    "transformations = [\"word-swap-embedding\", \"word-swap-hownet\", \"word-swap-wordnet\"]\n",
    "constraint_levels = [\"strict\"]\n",
    "search_methods = [\"tabu2\",\n",
    "                  \"tabu4\",\n",
    "                  \"tabu8\",\n",
    "                  \"tabu_dynamic_tenure\",\n",
    "                  \"tabu_dynamic_tenure8\",\n",
    "                  \"tabu_dynamic\",\n",
    "                  \"tabu_hdbscan4\",\n",
    "                  \"tabu_hdbscan8\",\n",
    "                  \"tabu_semantic_similarity4\",\n",
    "                  \"tabu_semantic_similarity8\"\n",
    "                  ]\n",
    "search_method_names = {\n",
    "    \"tabu2\": \"Tabu Search [tabu_size=2,tabu_tenure=2]\",\n",
    "    \"tabu4\": \"Tabu Search [tabu_size=4,tabu_tenure=4]\",\n",
    "    \"tabu8\": \"Tabu Search [tabu_size=8,tabu_tenure=8]\",\n",
    "    \"tabu_dynamic_tenure\": \"Tabu Search Dynamic Tenure [tabu_size=4]\",\n",
    "    \"tabu_dynamic_tenure8\": \"Tabu Search Dynamic Tenure [tabu_size=8]\",\n",
    "    \"tabu_dynamic\": \"Tabu Search Dynamic\",\n",
    "    \"tabu_hdbscan4\": \"Tabu Search HDBSCAN [n_clusters=4]\",\n",
    "    \"tabu_hdbscan8\": \"Tabu Search HDBSCAN [n_clusters=8]\",\n",
    "    \"tabu_semantic_similarity4\": \"Tabu Search Semantic Similarity [tabu_size=4, threshold=0.5]\",\n",
    "    \"tabu_semantic_similarity8\": \"Tabu Search Semantic Similarity [tabu_size=8, threshold=0.5]\"\n",
    "}\n",
    "\n",
    "RESULT_ROOT_DIR = \"./results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 33/150 [00:00<00:00, 161.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file ./results/bert-yelp-test/word-swap-embedding/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/bert-yelp-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "Error processing file ./results/bert-yelp-test/word-swap-hownet/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/bert-yelp-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "Error processing file ./results/bert-yelp-test/word-swap-wordnet/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/bert-yelp-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-mr-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-mr-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 81/150 [00:00<00:00, 180.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-mr-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu8.csv. Skipping this file.\n",
      "Error processing file ./results/lstm-yelp-test/word-swap-embedding/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-hownet/strict/tabu2.csv. Skipping this file.\n",
      "Error processing file ./results/lstm-yelp-test/word-swap-hownet/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/lstm-yelp-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu8.csv. Skipping this file.\n",
      "Error processing file ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu_dynamic.csv: Columns must be same length as key\n",
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-mr-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 102/150 [00:00<00:00, 190.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-mr-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-mr-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/150 [00:01<00:33,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-yelp-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 14/150 [00:02<00:20,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-yelp-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/150 [00:03<00:17,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-yelp-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 32/150 [00:04<00:14,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-mr-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 45/150 [00:04<00:10,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-mr-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 49/150 [00:04<00:09, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/bert-mr-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-embedding/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-hownet/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic_tenure.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic_tenure8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_dynamic.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_hdbscan8.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_semantic_similarity4.csv. Skipping this file.\n",
      "File not found: ./results/bert-snli-test/word-swap-wordnet/strict/tabu_semantic_similarity8.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu2.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu8.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 53/150 [00:05<00:10,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-yelp-test/word-swap-hownet/strict/tabu2.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 57/150 [00:07<00:11,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-yelp-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 61/150 [00:09<00:13,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu4.csv. Skipping this file.\n",
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu8.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 65/150 [00:10<00:14,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-yelp-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 73/150 [00:12<00:13,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-mr-test/word-swap-embedding/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 81/150 [00:13<00:11,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-mr-test/word-swap-hownet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 89/150 [00:15<00:10,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./results/lstm-mr-test/word-swap-wordnet/strict/tabu_hdbscan4.csv. Skipping this file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 93/150 [00:16<00:10,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "all_successful_attacks = []\n",
    "num_files = len(models) * len(transformations) * len(constraint_levels) * len(search_methods)\n",
    "pbar = tqdm.tqdm(total=num_files, smoothing=0)\n",
    "epsilon = 1e-10  # Small value to avoid division by zero\n",
    "\n",
    "# First, populate all_successful_attacks\n",
    "for model in models:\n",
    "    for t in transformations:\n",
    "        for cl in constraint_levels:\n",
    "            all_successful = set()\n",
    "            for sm in search_methods:\n",
    "                try:\n",
    "                    csv_path = f\"{RESULT_ROOT_DIR}/{model}/{t}/{cl}/{sm}.csv\"\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    df = df[df['result_type'] == \"Successful\"]\n",
    "                    df[\"original_text\"] = df.apply(lambda row: row[\"original_text\"].replace(\"[\", \"\").replace(\"]\", \"\"), axis=1)\n",
    "                    if len(all_successful) == 0:\n",
    "                        all_successful = set(df[\"original_text\"])\n",
    "                    else:\n",
    "                        all_successful = all_successful.intersection(set(df[\"original_text\"]))\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {csv_path}. Skipping this file.\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {csv_path}: {e}\")\n",
    "                    pass\n",
    "                pbar.update(1)\n",
    "            all_successful_attacks.append(all_successful)\n",
    "pbar.close()\n",
    "\n",
    "# Open the file in append mode\n",
    "with open('results.txt', 'a') as f:\n",
    "    # Then, evaluate and print results\n",
    "    num_files = len(models) * len(transformations) * len(constraint_levels) * len(search_methods)\n",
    "    pbar = tqdm.tqdm(total=num_files, smoothing=0)\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        for t in transformations:\n",
    "            for cl in constraint_levels:\n",
    "                f.write(\"=\"*45 + \"\\n\")\n",
    "                f.write(f\"{model}/{t}/{cl}\\n\")\n",
    "                f.write(\"-\"*45 + \"\\n\")\n",
    "                for sm in search_methods:\n",
    "                    try:\n",
    "                        csv_path = f\"{RESULT_ROOT_DIR}/{model}/{t}/{cl}/{sm}.csv\"\n",
    "                        all_successful = all_successful_attacks[i]\n",
    "                        avg_sim, avg_pp_diff, words_changed_percent = evaluator.evaluate(csv_path, all_successful)\n",
    "                        avg_sim = avg_sim / (len(all_successful) + epsilon)\n",
    "                        avg_pp_diff = avg_pp_diff / (len(all_successful) + epsilon)\n",
    "                        words_changed_percent = words_changed_percent / (len(all_successful) + epsilon)\n",
    "                        output_line = f\"sm: {sm}\\t  Word Changed Percent: {round(words_changed_percent, 2)} \\t USE Sim: {round(avg_sim, 3)} \\t PP Diff: {str(round(avg_pp_diff * 100, 1))}\\n\"\n",
    "                        f.write(output_line)\n",
    "                        pbar.update(1)\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {csv_path}. Skipping this file.\")\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        error_line = f\"Error evaluating {csv_path}: {e}\\n\"\n",
    "                        f.write(error_line)\n",
    "                        pass\n",
    "                i += 1\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
